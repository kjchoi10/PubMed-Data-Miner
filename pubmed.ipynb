{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PubMed Data Mining Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a data mining algorithm that scraps article citation information from PubMed (NCBI). The data is organized into pandas, and has natural language processing applied to research affliation data to pinpoint the location of the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import timeit\n",
    "from collections import defaultdict\n",
    "from geopy.geocoders import Nominatim\n",
    "from mpl_toolkits.basemap import Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### Functions: Methods: ###\n",
    "\n",
    "# setting up with your email\n",
    "class setup():\n",
    "    # setup the email\n",
    "    def email(self):\n",
    "        email = input(\"Please enter your email: \")\n",
    "        print (\"You enter: \", email)\n",
    "        return(email)\n",
    "    # returns list of keywords you are interested in\n",
    "    def keyword(self, numbers):\n",
    "        keyword_lst = []\n",
    "        while len(keyword_lst) != numbers:\n",
    "            keyword = input(\"Input keyword: \")\n",
    "            keyword_lst.append(keyword)\n",
    "        return(keyword_lst)\n",
    "    \n",
    "# retrieve the data\n",
    "class retrieve():\n",
    "    # returns list of keywords you are interested in\n",
    "    def keyword(self, numbers):\n",
    "        keyword_lst = []\n",
    "        while len(keyword_lst) != numbers:\n",
    "            keyword = input(\"Input keyword: \")\n",
    "            keyword_lst.append(keyword)\n",
    "        return(keyword_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting data from Medline: journals, authors, location\n",
    "def record_fetch(pmid):\n",
    "    # length of pmid - test sake\n",
    "    print (\"The length of pmid is\", len(pmid))\n",
    "\n",
    "    # timer\n",
    "    start_time_1 = timeit.default_timer()\n",
    "\n",
    "    # fetching pubmed articles using pmid ids\n",
    "    fetch_handle = Entrez.efetch(db=\"pubmed\", id=pmid, rettype=\"medline\",retmode=\"text\")\n",
    "\n",
    "    # timer\n",
    "    print (\"The handle fetch program time is \", timeit.default_timer() - start_time_1)\n",
    "\n",
    "    # timer\n",
    "    start_time_2 = timeit.default_timer()\n",
    "\n",
    "    # parsing Medline\n",
    "    records = Medline.parse(fetch_handle)\n",
    "    records = list(records)\n",
    "\n",
    "    # timer\n",
    "    print (\"The Medline parse program time is \", timeit.default_timer() - start_time_2)\n",
    "\n",
    "    # timer\n",
    "    start_time_3 = timeit.default_timer()\n",
    "\n",
    "    # init data columns\n",
    "    record_authors = []\n",
    "    record_first = []\n",
    "    record_journals = []\n",
    "    record_dp = []\n",
    "    record_place = []\n",
    "    record_mesh = []\n",
    "    record_pmid = []\n",
    "    record_aff = []\n",
    "\n",
    "    # iterate over records - try and errors are there to catch None types\n",
    "    for record in records:\n",
    "        try:\n",
    "            record_authors.append(record.get(\"AU\"))\n",
    "        except TypeError:\n",
    "            record_authors.append(\"None\")\n",
    "        try:\n",
    "            record_first.append(record.get(\"AU\")[0])\n",
    "        except TypeError:\n",
    "            record_first.append(\"None\")\n",
    "        try:\n",
    "            record_journals.append(record.get(\"JT\"))\n",
    "        except TypeError:\n",
    "            record_journals.append(\"None\")\n",
    "        try:\n",
    "            record_dp.append(record.get(\"DP\"))\n",
    "        except TypeError:\n",
    "            record_dp.append(\"None\")\n",
    "        try:\n",
    "            record_place.append(record.get(\"PL\"))\n",
    "        except TypeError:\n",
    "            record_place.append(\"None\")\n",
    "        try:\n",
    "            record_mesh.append(record.get(\"MH\"))\n",
    "        except TypeError:\n",
    "            record_mesh.append(\"None\")\n",
    "        try:\n",
    "            record_pmid.append(record.get(\"PMID\"))\n",
    "        except TypeError:\n",
    "            record_pmid.append(\"None\")\n",
    "        try:\n",
    "            record_aff.append(record.get(\"AD\"))\n",
    "        except TypeError:\n",
    "            record_aff.append(\"None\")\n",
    "\n",
    "    # timer\n",
    "    print (\"The author program time is \", timeit.default_timer() - start_time_3)\n",
    "\n",
    "    # data frame\n",
    "    data = pd.DataFrame({'authors': record_authors, 'firstauthor': record_first, 'journals': record_journals, 'datepublication': record_dp,\n",
    "    'place': record_place, 'mesh': record_mesh, 'pmid': record_pmid,'affliation': record_aff}, columns=['authors', 'firstauthor', 'journals',\n",
    "    'datepublication', 'place', 'mesh', 'pmid', 'affliation'])\n",
    "\n",
    "    # dropping miss data (None types)\n",
    "    data = data.dropna()\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# matching, analysis, and graph algorithms\n",
    "def analysis(keywords, date):\n",
    "    article_lst = []\n",
    "    ############################\n",
    "    # [ initial handler setup ]\n",
    "    start_time_final = timeit.default_timer()\n",
    "    for word in range(len(keywords)):\n",
    "        print(keywords[word])\n",
    "        # to find keyword population with count\n",
    "        start_time = timeit.default_timer()\n",
    "        #search_handle = Entrez.esearch(db=\"pubmed\",term=keywords[word],retmax=1, reldata=date, usehistory=\"y\")\n",
    "        search_handle = Entrez.esearch(db=\"pubmed\",term=keywords[word],retmax=1, mindate='2015/01', maxdate='2016/01', usehistory=\"y\")\n",
    "        #handle = Entrez.esearch(db=\"pubmed\",term=keywords[0], retmax=1)\n",
    "        #handle = Entrez.esearch(db=\"pubmed\",term=keywords[1],retmax=1)\n",
    "        search_results = Entrez.read(search_handle)\n",
    "        count = search_results['Count']\n",
    "\n",
    "        if int(count) <= 10000:\n",
    "            sample = 10000\n",
    "        else:\n",
    "            sample = float(count) * 0.30\n",
    "            sample = int(sample)\n",
    "\n",
    "        ### 1. I want to download the full sample of data (will take a long time) for the specified year\n",
    "        # 2. Then randomly select 1000 articles for that year\n",
    "\n",
    "        # 1. Date specified\n",
    "\n",
    "        # taking 10% of population and using this as sample\n",
    "        #search_handle= Entrez.esearch(db=\"pubmed\",term=keywords[word],retmax=sample, reldata=date, usehistory=\"y\")\n",
    "        search_handle = Entrez.esearch(db=\"pubmed\",term=keywords[word],retmax=sample, mindate='2015/01', maxdate='2016/01', usehistory=\"y\")\n",
    "        search_results = Entrez.read(search_handle)\n",
    "\n",
    "        # web servery history parameters\n",
    "        webenv = search_results[\"WebEnv\"]\n",
    "        query_key = search_results[\"QueryKey\"]\n",
    "\n",
    "        print (\"The initial handle program time: \", timeit.default_timer() - start_time)\n",
    "\n",
    "        ############################\n",
    "        # [ handle test data ]\n",
    "        #handle = Entrez.esearch(db=\"pubmed\",term=keywords[0],retmax=sample)\n",
    "        #handle = Entrez.esearch(db=\"pubmed\",term=keywords[1],retmax=sample)\n",
    "\n",
    "        print (\"Sample size is \", sample)\n",
    "\n",
    "        # storing the sample as the idlist for the articles\n",
    "\n",
    "        ############################\n",
    "        # [ splitting step ]\n",
    "        start_time_2 = timeit.default_timer()\n",
    "        id_results = search_results[\"IdList\"]\n",
    "        pmid = id_results\n",
    "        pmid = sorted(pmid)\n",
    "        print (\"The sort program time \", timeit.default_timer() - start_time_2)\n",
    "        test = record_fetch(pmid)\n",
    "        article_lst.append(test)\n",
    "    print (\"The final program time: \", timeit.default_timer() - start_time_final)\n",
    "    return(article_lst)\n",
    "\n",
    "# Making csv files for each keyword in the data frame\n",
    "def make_csv(df):\n",
    "    for idx, val  in enumerate(df):\n",
    "        val.to_csv('%s.csv' % keywords[idx], sep=',')\n",
    "        \n",
    "# Make each of the csv files into a listed dataframe\n",
    "def make_df(keywords):\n",
    "    df = []\n",
    "    for idx, val in enumerate(keywords):\n",
    "        df_temp = pd.read_csv('%s.csv' % keywords[idx])\n",
    "        df.append(df_temp)\n",
    "    return(df)\n",
    "\n",
    "# random sample of 1000 for each data frame of keywords\n",
    "def make_sampled(df):\n",
    "    for idx, val in enumerate(df):\n",
    "        if len(df[idx]) >= 1000:\n",
    "            df[idx] = val.sample(n=1000)\n",
    "        else:\n",
    "            df[idx] = val.sample(n=len(df[idx]))\n",
    "    return(df)\n",
    "\n",
    "# matching articles\n",
    "def matching(df, keywords):\n",
    "    try:\n",
    "        articles = [item['pmid'] for item in df]\n",
    "        #print(\"list\")\n",
    "    except TypeError:\n",
    "        articles = df['pmid']  \n",
    "        articles = list(articles)\n",
    "        #print(\"pandas\")\n",
    "         \n",
    "    weights = []\n",
    "    combos = []\n",
    "    combos_lst = []\n",
    "    weighted_el_lst = []\n",
    "    weighted_el_ = []\n",
    "    for article in range(len(articles)):\n",
    "        #if len(list(set(article) & set(article + 1))) != 0:\n",
    "        if article is not len(articles) - 1 : # does not match the last index\n",
    "            for index in range(len(articles)):\n",
    "                if (index + article) <= len(articles) - 1: # article number + index does not equal the full length of articles\n",
    "                    #print(article, index)\n",
    "                    if len(list(set(articles[article]).intersection(set(articles[article + index])))) != 0: # if there are any matches\n",
    "                        #empty list to make weighted edges form: (edges, edge, weights)\n",
    "                        combos_lst = []\n",
    "                        weights_lst = []\n",
    "\n",
    "                        combo = tuple([keywords[article]] + [keywords[article + index]])\n",
    "                        combos.append(combo)\n",
    "\n",
    "                        #combos for weighted edges\n",
    "                        combo_lst = keywords[article]\n",
    "                        combos_lst.append(combo_lst)\n",
    "                        combo_lst2 = keywords[article + index]\n",
    "                        combos_lst.append(combo_lst2)\n",
    "\n",
    "                        weight = len(list(set(articles[article]).intersection(set(articles[article + index]))))\n",
    "                        weights.append(weight)\n",
    "\n",
    "                        weighted_el = zip(combos, weights)\n",
    "                        weighted_el_.append(weighted_el)\n",
    "\n",
    "                        #for weighted edges\n",
    "                        weights_lst.append(weight)\n",
    "                        weighted_el_tup = tuple(combos_lst + weights_lst)\n",
    "                        weighted_el_lst.append(weighted_el_tup)\n",
    "\n",
    "    return(weights, combos, weighted_el_, weighted_el_lst)\n",
    "\n",
    "\n",
    "# make a weighted graph of the keywords with the keyword of interest\n",
    "def target_graph(weighted_el_lst):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # specify the keyword that you're interested in\n",
    "    interested_keyword = input(\"Please enter the keyword you are interested in: \")\n",
    "    print (\"You enter: \", interested_keyword)\n",
    "\n",
    "    weighted_edgelist = match[3]\n",
    "\n",
    "    # only choose the combos that include the interested keyword\n",
    "    graph_of_interest = [interest for interest in weighted_edgelist if interested_keyword in interest]\n",
    "\n",
    "    G.add_weighted_edges_from(graph_of_interest)\n",
    "    pos = nx.spring_layout(G)\n",
    "    custom_labels = {}\n",
    "    nodes = G.nodes()\n",
    "    for labels in nodes:\n",
    "        custom_labels[labels] = labels\n",
    "\n",
    "    edge_weight=dict([((u,v,),int(d['weight'])) for u,v,d in G.edges(data=True)])\n",
    "\n",
    "    nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_weight)\n",
    "    nx.draw_networkx_nodes(G,pos)\n",
    "    nx.draw_networkx_edges(G,pos)\n",
    "    nx.draw_networkx_labels(G,pos)\n",
    "    plt.show()\n",
    "\n",
    "# make a weighted graph of the keywords\n",
    "def all_graph(weighted_el_lst):\n",
    "    G = nx.Graph()\n",
    "    preds = nx.adamic_adar_index(G, match[1])\n",
    "\n",
    "    weighted_edgelist = match[3]\n",
    "    G.add_weighted_edges_from(weighted_edgelist)\n",
    "    preds = nx.adamic_adar_index(G, match[1])\n",
    "    pos = nx.spring_layout(G)\n",
    "    custom_labels = {}\n",
    "    nodes = G.nodes()\n",
    "    for labels in nodes:\n",
    "        custom_labels[labels] = labels\n",
    "\n",
    "    edge_weight=dict([((u,v,),int(d['weight'])) for u,v,d in G.edges(data=True)])\n",
    "\n",
    "    nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_weight)\n",
    "    nx.draw_networkx_nodes(G,pos)\n",
    "    nx.draw_networkx_edges(G,pos)\n",
    "    nx.draw_networkx_labels(G,pos)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# mapping  countries\n",
    "def map_countries(data):\n",
    "    geolocator = Nominatim()\n",
    "    location = [geolocator.geocode(country) for country in unique_countries]\n",
    "    #institution = [geolocator.geocode(affliation) for affliation in unique_affliations]\n",
    "    print(location)\n",
    "    # make sure the value of resolution is a lowercase L,\n",
    "    #  for 'low', not a numeral 1\n",
    "    my_map = Basemap(projection='robin', lat_0=0, lon_0=-100, resolution='l', area_thresh=1000.0)\n",
    "\n",
    "    my_map.drawcoastlines()\n",
    "    my_map.drawcountries()\n",
    "    my_map.fillcontinents(color='coral')\n",
    "    my_map.drawmapboundary()\n",
    "\n",
    "    my_map.drawmeridians(np.arange(0, 360, 30))\n",
    "    my_map.drawparallels(np.arange(-90, 90, 30))\n",
    "\n",
    "    lons = [(location[lon][1][1]) for lon in range(len(location))]\n",
    "    lats = [(location[lat][1][0]) for lat in range(len(location))]\n",
    "    x,y = my_map(lons, lats)\n",
    "    my_map.plot(x, y, 'bo', markersize=10)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your email: kjchoi10@gmail.com\n",
      "You enter:  kjchoi10@gmail.com\n"
     ]
    }
   ],
   "source": [
    "### 1: Setup\n",
    "\n",
    "# Must run this and enter your email for NCBI\n",
    "init = setup()\n",
    "Entrez.email = init.email()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example keyword list\n",
    "keywords = [\"Robotic Surgery\", \"Cardiac Surgery\", \"Colorectal Surgery\", \"General Surgery\", \"Gynecologic Surgery\", \"Head & Neck Surgery\",\"Thoracic Surgery\", \"Urologic Surgery\"]\n",
    "\n",
    "# Number of days in the past you want to look at (past 5 years)\n",
    "date = 1825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robotic Surgery\n",
      "The initial handle program time:  1.0315892290018382\n",
      "Sample size is  10000\n",
      "The sort program time  0.00013467100143316202\n",
      "The length of pmid is 1620\n",
      "The handle fetch program time is  0.723740966997866\n",
      "The Medline parse program time is  26.098299224002403\n",
      "The author program time is  0.005180276999453781\n",
      "Cardiac Surgery\n",
      "The initial handle program time:  1.715174503999151\n",
      "Sample size is  5697\n",
      "The sort program time  0.0011230220006837044\n",
      "The length of pmid is 5697\n",
      "The handle fetch program time is "
     ]
    }
   ],
   "source": [
    "# Analysis: returns a pandas data frame\n",
    "# outputs are the keywords, sample size, and timing for each keyword\n",
    "df = analysis(keywords, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the covariate names more explicity for the data frame\n",
    "articles = [item['pmid'] for item in df]\n",
    "place = [item['place'] for item in df]\n",
    "date = [item['datepublication'] for item in df]\n",
    "affliation = [item['affliation'] for item in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Finds all matching articles between keywords\n",
    "match = matching(df, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### plotting graphs:\n",
    "# Place for first keyword\n",
    "place = df[0]['place']\n",
    "place = map(lambda x: x.upper(), place)\n",
    "df[0]['place'] = map(lambda x: x.upper(), df[0]['place']) # to avoid duplicates of country names\n",
    "unique_countries = pd.unique(df[0].place.ravel()) # getting the unique country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### plotting the graphs:\n",
    "# Affliation fo first keyword\n",
    "df[0]['affliation'] = map(lambda x: x.upper(), df[0]['affliation'])\n",
    "affliation = df[0]['affliation']\n",
    "unique_affliations = pd.unique(df[0].affliation.ravel()) # getting the unique country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Divisions of Cardiovascular Surgery and Cardiology, Peter Munk Cardiac Centre, Toronto General Hospital, University of Toronto, Toronto, Ontario, Canada. Electronic address: tirone.david@uhn.ca. Divisions of Cardiovascular Surgery and Cardiology, Peter Munk Cardiac Centre, Toronto General Hospital, University of Toronto, Toronto, Ontario, Canada. Divisions of Cardiovascular Surgery and Cardiology, Peter Munk Cardiac Centre, Toronto General Hospital, University of Toronto, Toronto, Ontario, Canada. Divisions of Cardiovascular Surgery and Cardiology, Peter Munk Cardiac Centre, Toronto General Hospital, University of Toronto, Toronto, Ontario, Canada. Divisions of Cardiovascular Surgery and Cardiology, Peter Munk Cardiac Centre, Toronto General Hospital, University of Toronto, Toronto, Ontario, Canada. Divisions of Cardiovascular Surgery and Cardiology, Peter Munk Cardiac Centre, Toronto General Hospital, University of Toronto, Toronto, Ontario, Canada.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[1]['affliation'][51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Testing affliation geolocation\n",
    "import nltk\n",
    "test = df[1]['affliation'][45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From the Department of Radiology and Research Institute of Radiology, University of Ulsan College of Medicine, Asan Medical Center, Seoul, Korea (HX, HJK, SL, JWL, HNL, MYK); Department of Radiology, The First Affiliated Hospital of Nanjing Medical University, Nanjing, Jiangsu Province, China (HX); Department of Thoracic and Cardiovascular Surgery (DKK); and Pathology, University of Ulsan College of Medicine, Asan Medical Center, Seoul, Korea (JSS).'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token = test.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " 'the',\n",
       " 'Department',\n",
       " 'of',\n",
       " 'Radiology',\n",
       " 'and',\n",
       " 'Research',\n",
       " 'Institute',\n",
       " 'of',\n",
       " 'Radiology',\n",
       " ',',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Ulsan',\n",
       " 'College',\n",
       " 'of',\n",
       " 'Medicine',\n",
       " ',',\n",
       " 'Asan',\n",
       " 'Medical',\n",
       " 'Center',\n",
       " ',',\n",
       " 'Seoul',\n",
       " ',',\n",
       " 'Korea',\n",
       " '(',\n",
       " 'HX',\n",
       " ',',\n",
       " 'HJK',\n",
       " ',',\n",
       " 'SL',\n",
       " ',',\n",
       " 'JWL',\n",
       " ',',\n",
       " 'HNL',\n",
       " ',',\n",
       " 'MYK',\n",
       " ');',\n",
       " 'Department',\n",
       " 'of',\n",
       " 'Radiology',\n",
       " ',',\n",
       " 'The',\n",
       " 'First',\n",
       " 'Affiliated',\n",
       " 'Hospital',\n",
       " 'of',\n",
       " 'Nanjing',\n",
       " 'Medical',\n",
       " 'University',\n",
       " ',',\n",
       " 'Nanjing',\n",
       " ',',\n",
       " 'Jiangsu',\n",
       " 'Province',\n",
       " ',',\n",
       " 'China',\n",
       " '(',\n",
       " 'HX',\n",
       " ');',\n",
       " 'Department',\n",
       " 'of',\n",
       " 'Thoracic',\n",
       " 'and',\n",
       " 'Cardiovascular',\n",
       " 'Surgery',\n",
       " '(',\n",
       " 'DKK',\n",
       " ');',\n",
       " 'and',\n",
       " 'Pathology',\n",
       " ',',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Ulsan',\n",
       " 'College',\n",
       " 'of',\n",
       " 'Medicine',\n",
       " ',',\n",
       " 'Asan',\n",
       " 'Medical',\n",
       " 'Center',\n",
       " ',',\n",
       " 'Seoul',\n",
       " ',',\n",
       " 'Korea',\n",
       " '(',\n",
       " 'JSS',\n",
       " ').']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize \n",
    "tokened = wordpunct_tokenize(test)\n",
    "tokened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how to get the location of the research\n",
    "import string\n",
    "# removes the punctations in the tokened array\n",
    "tokened = [''.join(c for c in s if c not in string.punctuation) for s in tokened]\n",
    "tokened = [s for s in tokened if s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Department', 'Radiology', 'Research', 'Institute', 'Radiology', 'University', 'Ulsan', 'College', 'Medicine', 'Asan', 'Medical', 'Center', 'Seoul', 'Korea', 'HX', 'HJK', 'SL', 'JWL', 'HNL', 'MYK', 'Department', 'Radiology', 'First', 'Affiliated', 'Hospital', 'Nanjing', 'Medical', 'University', 'Nanjing', 'Jiangsu', 'Province', 'China', 'HX', 'Department', 'Thoracic', 'Cardiovascular', 'Surgery', 'DKK', 'Pathology', 'University', 'Ulsan', 'College', 'Medicine', 'Asan', 'Medical', 'Center', 'Seoul', 'Korea', 'JSS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Radiology'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get rid of noise_list_words\n",
    "def clean_phrase(data):\n",
    "    try:\n",
    "        tokened = [number for number in df[1]['affliation'][number]]\n",
    "    except:\n",
    "    tokened = wordpunct_tokenize(test)\n",
    "    noise_words_set = ['of', 'the', 'in', 'for', 'at', 'and', 'from', 'com', 'org', 'MD']\n",
    "    clean = [phrase for phrase in tokened if phrase.lower() not in noise_words_set]\n",
    "\n",
    "# working\n",
    "noise_words_set = ['of', 'the', 'in', 'for', 'at', 'and', 'from', 'com', 'org', 'MD']\n",
    "stuff = [phrase for phrase in tokened if phrase.lower() not in noise_words_set]\n",
    "print(stuff)\n",
    "location_set = max(set(stuff), key=stuff.count)\n",
    "location_set\n",
    "# use a database of city names to match the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim()\n",
    "location = geolocator.geocode(location_set)\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From the Department of Radiology and Research Institute of Radiology, University of Ulsan College of Medicine, Asan Medical Center, Seoul, Korea (HX, HJK, SL, JWL, HNL, MYK); Department of Radiology, The First Affiliated Hospital of Nanjing Medical University, Nanjing, Jiangsu Province, China (HX); Department of Thoracic and Cardiovascular Surgery (DKK); and Pathology, University of Ulsan College of Medicine, Asan Medical Center, Seoul, Korea (JSS).'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
